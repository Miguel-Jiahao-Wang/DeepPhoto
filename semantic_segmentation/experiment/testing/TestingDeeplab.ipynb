{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"101DEEPLAB.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ca8hm5nOJLRp","colab_type":"text"},"source":["# Assessing performance of DeeplabV3 on validation set"]},{"cell_type":"markdown","metadata":{"id":"_yakwHYyL0nM","colab_type":"text"},"source":["## Setup and imports"]},{"cell_type":"code","metadata":{"id":"UYKGBH4-31uu","colab_type":"code","outputId":"0914d67e-4371-4161-9623-427ad10020dd","executionInfo":{"status":"ok","timestamp":1573479395456,"user_tz":-60,"elapsed":27544,"user":{"displayName":"Jørgen Lund","photoUrl":"","userId":"14761813338264063760"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s77TdXYH5Yf5","colab_type":"code","outputId":"f083e0ee-4dae-41f7-d6d2-9ed09cb101bf","executionInfo":{"status":"ok","timestamp":1573479404494,"user_tz":-60,"elapsed":3257,"user":{"displayName":"Jørgen Lund","photoUrl":"","userId":"14761813338264063760"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd /content/drive/My Drive/DeepLearningX/gitclone/light-weight-refinenet/src"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/DeepLearningX/gitclone/light-weight-refinenet/src\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C-X2w5mCvJv1","colab_type":"code","colab":{}},"source":["# general libs\n","import argparse\n","import logging\n","import os\n","import random\n","import re\n","import sys\n","import time\n","\n","# misc\n","import cv2\n","import numpy as np\n","\n","# pytorch libs\n","import torch\n","import torch.nn as nn\n","from sklearn.metrics import confusion_matrix\n","\n","# custom libs\n","from util import *"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5k1z5ATAL2LK","colab_type":"text"},"source":["## Methods for training"]},{"cell_type":"code","metadata":{"id":"UIIwnxyc-ma8","colab_type":"code","colab":{}},"source":["# defining methods for validation\n","sys.path.append(\"..\")\n","\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, random_split\n","# Custom libraries\n","from datasets import NYUDataset as Dataset\n","from datasets import Pad, RandomCrop, RandomMirror, ResizeShorterScale, ToTensor, Normalise\n","\n","def create_segmenter(\n","    net, pretrained, num_classes\n","    ):\n","    \"\"\"Create Encoder; for now only ResNet [50,101,152]\"\"\"\n","    from models.resnet import rf_lw50, rf_lw101, rf_lw152\n","    if str(net) == '50':\n","        return rf_lw50(num_classes, imagenet=pretrained)\n","    elif str(net) == '101':\n","        return rf_lw101(num_classes, imagenet=pretrained)\n","    elif str(net) == '152':\n","        return rf_lw152(num_classes, imagenet=pretrained)\n","    elif str(net) == 'Mob':\n","        return mbv2(num_classes, pretrained=pretrained)\n","    elif str(net) == \"Deep\":\n","        return torchvision.models.segmentation.deeplabv3_resnet101(pretrained=pretrained)\n","    else:\n","        raise ValueError(\"{} is not supported\".format(str(net)))\n","\n","def create_loaders(\n","    train_dir, val_dir, train_list, val_list,\n","    shorter_side, crop_size, low_scale, high_scale,\n","    normalise_params, batch_size, num_workers, ignore_label\n","    ):\n","    \"\"\"\n","    Args:\n","      train_dir (str) : path to the root directory of the training set.\n","      val_dir (str) : path to the root directory of the validation set.\n","      train_list (str) : path to the training list.\n","      val_list (str) : path to the validation list.\n","      shorter_side (int) : parameter of the shorter_side resize transformation.\n","      crop_size (int) : square crop to apply during the training.\n","      low_scale (float) : lowest scale ratio for augmentations.\n","      high_scale (float) : highest scale ratio for augmentations.\n","      normalise_params (list / tuple) : img_scale, img_mean, img_std.\n","      batch_size (int) : training batch size.\n","      num_workers (int) : number of workers to parallelise data loading operations.\n","      ignore_label (int) : label to pad segmentation masks with\n","\n","    Returns:\n","      train_loader, val loader\n","\n","    \"\"\"\n","\n","    ## Transformations during training ##\n","    composed_trn = transforms.Compose([ResizeShorterScale(shorter_side, low_scale, high_scale),\n","                                    Pad(crop_size, [123.675, 116.28 , 103.53], ignore_label),\n","                                    RandomMirror(),\n","                                    RandomCrop(crop_size),\n","                                    Normalise(*normalise_params),\n","                                    ToTensor()])\n","    composed_val = transforms.Compose([Normalise(*normalise_params),\n","                                    ToTensor()])\n","    ## Training and validation sets ##\n","    trainset = Dataset(data_file=train_list,\n","                       data_dir=train_dir,\n","                       transform_trn=composed_trn,\n","                       transform_val=composed_val)\n","\n","    valset = Dataset(data_file=val_list,\n","                         data_dir=val_dir,\n","                         transform_trn=None,\n","                         transform_val=composed_val)\n","    logger.info(\" Created train set = {} examples, val set = {} examples\"\n","                .format(len(trainset), len(valset)))\n","    ## Training and validation loaders ##\n","    train_loader = DataLoader(trainset,\n","                              batch_size=batch_size,\n","                              shuffle=True,\n","                              num_workers=num_workers,\n","                              pin_memory=True,\n","                              drop_last=True)\n","    val_loader = DataLoader(valset,\n","                            batch_size=1,\n","                            shuffle=False,\n","                            num_workers=num_workers,\n","                            pin_memory=True)\n","    return train_loader, val_loader\n","\n","def create_optimisers(\n","    lr_enc, lr_dec,\n","    mom_enc, mom_dec,\n","    wd_enc, wd_dec,\n","    param_enc, param_dec,\n","    optim_dec\n","    ):\n","    \"\"\"Create optimisers for encoder, decoder and controller\"\"\"\n","    optim_enc = torch.optim.SGD(param_enc, lr=lr_enc, momentum=mom_enc,\n","                                weight_decay=wd_enc)\n","    if optim_dec == 'sgd':\n","        optim_dec = torch.optim.SGD(param_dec, lr=lr_dec,\n","                                    momentum=mom_dec, weight_decay=wd_dec)\n","    elif optim_dec == 'adam':\n","        optim_dec = torch.optim.Adam(param_dec, lr=lr_dec, weight_decay=wd_dec, eps=1e-3)\n","    return optim_enc, optim_dec\n","\n","def load_ckpt(\n","    ckpt_path, ckpt_dict\n","    ):\n","    best_val = epoch_start = 0\n","    if os.path.exists(CKPT_PATH):\n","        ckpt = torch.load(ckpt_path)\n","        for (k, v) in ckpt_dict.items():\n","            if k in ckpt:\n","                v.load_state_dict(ckpt[k])\n","        best_val = ckpt.get('best_val', 0)\n","        epoch_start = ckpt.get('epoch_start', 0)\n","        logger.info(\" Found checkpoint at {} with best_val {:.4f} at epoch {}\".\n","            format(\n","                ckpt_path, best_val, epoch_start\n","            ))\n","    return best_val, epoch_start\n","\n","def train_segmenter(\n","    segmenter, train_loader, optim_enc, optim_dec,\n","    epoch, segm_crit, freeze_bn\n","    ):\n","    \"\"\"Training segmenter\n","\n","    Args:\n","      segmenter (nn.Module) : segmentation network\n","      train_loader (DataLoader) : training data iterator\n","      optim_enc (optim) : optimiser for encoder\n","      optim_dec (optim) : optimiser for decoder\n","      epoch (int) : current epoch\n","      segm_crit (nn.Loss) : segmentation criterion\n","      freeze_bn (bool) : whether to keep BN params intact\n","\n","    \"\"\"\n","    train_loader.dataset.set_stage('train')\n","    segmenter.train()\n","    if freeze_bn:\n","        for m in segmenter.modules():\n","            if isinstance(m, nn.BatchNorm2d):\n","                m.eval()\n","    batch_time = AverageMeter()\n","    losses = AverageMeter()\n","    for i, sample in enumerate(train_loader):\n","        start = time.time()\n","        input = sample['image'].cuda()\n","        target = sample['mask'].cuda()\n","        input_var = torch.autograd.Variable(input).float()\n","        target_var = torch.autograd.Variable(target).long()\n","        # Compute output\n","        output = segmenter(input_var)\n","        output = nn.functional.interpolate(output, size=target_var.size()[1:], mode='bilinear', align_corners=False)\n","        soft_output = nn.LogSoftmax()(output)\n","        # Compute loss and backpropagate\n","        loss = segm_crit(soft_output, target_var)\n","        optim_enc.zero_grad()\n","        optim_dec.zero_grad()\n","        loss.backward()\n","        optim_enc.step()\n","        optim_dec.step()\n","        losses.update(loss.item())\n","        batch_time.update(time.time() - start)\n","        if i % PRINT_EVERY == 0:\n","            logger.info(' Train epoch: {} [{}/{}]\\t'\n","                        'Avg. Loss: {:.3f}\\t'\n","                        'Avg. Time: {:.3f}'.format(\n","                            epoch, i, len(train_loader),\n","                            losses.avg, batch_time.avg\n","                        ))\n","def compute_iu(cm):\n","    \"\"\"Compute IU from confusion matrix.\n","\n","    Args:\n","      cm (Tensor) : square confusion matrix.\n","\n","    Returns:\n","      IU vector (Tensor).\n","\n","    \"\"\"\n","    pi = 0\n","    gi = 0\n","    ii = 0\n","    denom = 0\n","    n_classes = cm.shape[0]\n","    IU = np.ones(n_classes)\n","    \n","    for i in range(n_classes):\n","        pi = sum(cm[:, i])\n","        gi = sum(cm[i, :])\n","        ii = cm[i, i]\n","        denom = pi + gi - ii\n","        if denom > 0:\n","            IU[i] = ii / denom\n","    return IU\n","\n","def validate(\n","    segmenter, val_loader, epoch, num_classes=-1\n","    ):\n","    \"\"\"Validate segmenter\n","\n","    Args:\n","      segmenter (nn.Module) : segmentation network\n","      val_loader (DataLoader) : training data iterator\n","      epoch (int) : current epoch\n","      num_classes (int) : number of classes to consider\n","\n","    Returns:\n","      Mean IoU (float)\n","    \"\"\"\n","    val_loader.dataset.set_stage('val')\n","    segmenter.eval()\n","    cm = np.zeros((num_classes, num_classes), dtype=int)\n","\n","    times = []\n","    with torch.no_grad():\n","        for i, sample in enumerate(val_loader):\n","            start = time.time()\n","            input = sample['image']\n","            target = sample['mask']\n","            input_var = torch.autograd.Variable(input).float().cuda()\n","            # Compute output\n","            start = time.time()\n","            output = segmenter(input_var)\n","            end = time.time()\n","            times.append(end-start)\n","            output = cv2.resize(output[\"out\"][0][[0,15]].data.cpu().numpy().transpose(1, 2, 0),\n","                                target.size()[1:][::-1],\n","                                interpolation=cv2.INTER_CUBIC).argmax(axis=2).astype(np.uint8)\n","\n","            # Compute IoU\n","            gt = target[0].data.cpu().numpy().astype(np.uint8)\n","            gt_idx = gt < num_classes # Ignore every class index larger than the number of classes\n","            cm += confusion_matrix(output[gt_idx], gt[gt_idx])\n","\n","            # if i % PRINT_EVERY == 0:\n","            #     logger.info(' Val epoch: {} [{}/{}]\\t'\n","            #                 'Mean IoU: {:.3f}'.format(\n","            #                     epoch, i, len(val_loader),\n","            #                     compute_iu(cm).mean()\n","            #                 ))\n","\n","    ious = compute_iu(cm)\n","    logger.info(\" IoUs: {}\".format(ious))\n","    logger.info(\" Mean time: {}\".format(np.mean(times)))\n","    miou = np.mean(ious)\n","\n","    # miou_path = '/content/drive/My Drive/DeepLearningX/models/ResNet/mious_res{}_{}.txt'.format(ENC, FREEZED)\n","\n","    # with open(miou_path, 'a') as f:\n","    #   f.write(\"{}\\n\".format(miou))\n","\n","    logger.info(' Val epoch: {}\\tMean IoU: {:.3f}'.format(\n","                                epoch, miou))\n","    return miou\n","\n","def main():\n","    logging.basicConfig(level=logging.INFO)\n","    global logger #, args\n","    # args = get_arguments()\n","    logger = logging.getLogger(__name__)\n","    \n","    ## Add args ##\n","    NUM_STAGES = len(NUM_CLASSES)\n","\n","    ## Set random seeds ##\n","    torch.backends.cudnn.deterministic = True\n","    torch.manual_seed(RANDOM_SEED)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(RANDOM_SEED)\n","    np.random.seed(RANDOM_SEED)\n","    random.seed(RANDOM_SEED)\n","    \n","    ## Generate Segmenter ##\n","    segmenter = nn.DataParallel(\n","        create_segmenter(ENC, ENC_PRETRAINED, NUM_CLASSES[0])\n","        ).cuda()\n","\n","    # segmenter = create_segmenter(ENC, ENC_PRETRAINED, NUM_CLASSES[0]).cuda()\n","    logger.info(\" Loaded Segmenter {}, ImageNet-Pre-Trained={}, #PARAMS={:3.2f}M\"\n","                .format(ENC, ENC_PRETRAINED, compute_params(segmenter) / 1e6))\n","    \n","    ## Restore if any ## (at checkpoint)\n","    best_val, epoch_start = load_ckpt(CKPT_PATH, {'segmenter' : segmenter})\n","    \n","    ## Criterion ##\n","    segm_crit = nn.NLLLoss2d(ignore_index=IGNORE_LABEL).cuda()\n","\n","    # ## Saver ##\n","    # saver = Saver(args=vars(args),\n","    #               ckpt_dir=SNAPSHOT_DIR,\n","    #               best_val=best_val,\n","    #               condition=lambda x, y: x > y)  # keep checkpoint with the best validation score\n","\n","    logger.info(\" Training Process Starts\")\n","    for task_idx in range(NUM_STAGES):\n","        start = time.time()\n","        torch.cuda.empty_cache()\n","        ## Create dataloaders ##\n","        train_loader, val_loader = create_loaders(TRAIN_DIR,\n","                                                  VAL_DIR,\n","                                                  TRAIN_LIST[task_idx],\n","                                                  VAL_LIST[task_idx],\n","                                                  SHORTER_SIDE[task_idx],\n","                                                  CROP_SIZE[task_idx],\n","                                                  LOW_SCALE[task_idx],\n","                                                  HIGH_SCALE[task_idx],\n","                                                  NORMALISE_PARAMS,\n","                                                  BATCH_SIZE[task_idx],\n","                                                  NUM_WORKERS,\n","                                                  IGNORE_LABEL)\n","        if EVALUATE:\n","            return validate(segmenter, val_loader, 0, num_classes=NUM_CLASSES[task_idx])\n","\n","        logger.info(\" Training Stage {}\".format(str(task_idx)))\n","        ## Optimisers ##\n","        enc_params = []\n","        dec_params = []\n","        for k,v in segmenter.named_parameters():\n","            if bool(re.match(\".*conv1.*|.*bn1.*|.*layer.*\", k)):\n","                enc_params.append(v)\n","                logger.info(\" Enc. parameter: {}\".format(k))\n","            else:\n","                dec_params.append(v)\n","                logger.info(\" Dec. parameter: {}\".format(k))\n","        optim_enc, optim_dec = create_optimisers(LR_ENC[task_idx], LR_DEC[task_idx],\n","                                                 MOM_ENC[task_idx], MOM_DEC[task_idx],\n","                                                 WD_ENC[task_idx], WD_DEC[task_idx],\n","                                                 enc_params, dec_params, OPTIM_DEC)\n","        for epoch in range(NUM_SEGM_EPOCHS[task_idx]):\n","            train_segmenter(segmenter, train_loader,\n","                            optim_enc, optim_dec,\n","                            epoch_start, segm_crit,\n","                            FREEZE_BN[task_idx])\n","            if (epoch + 1) % (VAL_EVERY[task_idx]) == 0:\n","                miou = validate(segmenter, val_loader, epoch_start, NUM_CLASSES[task_idx])\n","                # saver.save(\n","                #     miou,\n","                #     {'segmenter' : segmenter.state_dict(),\n","                #      'epoch_start' : epoch_start}, logger\n","                #      )\n","            epoch_start += 1\n","            \n","            # torch.save(segmenter, \"/content/drive/My Drive/DeepLearningX/models/ResNet/model_res{}_{}_{}\".format(ENC, FREEZED, epoch))\n","\n","        logger.info(\"Stage {} finished, time spent {:.3f}min\".format(\n","            task_idx, (time.time() - start) / 60.))\n","        \n","    # logger.info(\"All stages are now finished. Best Val is {:.3f}\".format(\n","    #     saver.best_val))    \n","\n","# if __name__ == '__main__':\n","#     logging.basicConfig(level=logging.INFO)\n","#     main()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hdi1G-3IL4R0","colab_type":"text"},"source":["## Configurations and training"]},{"cell_type":"code","metadata":{"id":"mw-IF2QlzGXh","colab_type":"code","colab":{}},"source":["# DATASET PARAMETERS\n","TRAIN_DIR = \"/content/drive/My Drive/DeepLearningX/TrainData-People/Train/\"\n","VAL_DIR = \"/content/drive/My Drive/DeepLearningX/TrainData-People/Validation/\"\n","TRAIN_LIST = [\"/content/drive/My Drive/DeepLearningX/TrainData-People/Train/train.txt\"] * 3\n","VAL_LIST = [\"/content/drive/My Drive/DeepLearningX/TrainData-People/Validation/validation.txt\"] * 3\n","SHORTER_SIDE = [350] * 3\n","CROP_SIZE = [500] * 3\n","NORMALISE_PARAMS = [1./255, # SCALE\n","                    np.array([0.485, 0.456, 0.406]).reshape((1, 1, 3)), # MEAN\n","                    np.array([0.229, 0.224, 0.225]).reshape((1, 1, 3))] # STD\n","BATCH_SIZE = [10] * 3\n","NUM_WORKERS = 16\n","NUM_CLASSES = [2] * 3\n","LOW_SCALE = [0.5] * 3\n","HIGH_SCALE = [2.0] * 3\n","IGNORE_LABEL = 255\n","\n","# ENCODER PARAMETERS\n","ENC = 'Deep' # Which model we are training\n","ENC_PRETRAINED = True  # pre-trained on ImageNet or randomly initialised"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRzxpZ3OsDzV","colab_type":"code","outputId":"718672fb-670e-4d95-c405-54bc7fa223fa","executionInfo":{"status":"ok","timestamp":1573483012077,"user_tz":-60,"elapsed":44719,"user":{"displayName":"Jørgen Lund","photoUrl":"","userId":"14761813338264063760"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["segmenter = nn.DataParallel(\n","        create_segmenter(ENC, ENC_PRETRAINED, NUM_CLASSES[0])\n","        ).cuda()\n","\n","# valset = Dataset(data_file=val_list,\n","#                          data_dir=val_dir,\n","#                          transform_trn=None,\n","#                          transform_val=composed_val)\n","\n","task_idx = 0\n","\n","logging.basicConfig(level=logging.INFO)\n","global logger\n","logger = logging.getLogger(__name__)\n","\n","_, val_loader = create_loaders(TRAIN_DIR,\n","                            VAL_DIR,\n","                            TRAIN_LIST[task_idx],\n","                            VAL_LIST[task_idx],\n","                            SHORTER_SIDE[task_idx],\n","                            CROP_SIZE[task_idx],\n","                            LOW_SCALE[task_idx],\n","                            HIGH_SCALE[task_idx],\n","                            NORMALISE_PARAMS,\n","                            BATCH_SIZE[task_idx],\n","                            NUM_WORKERS,\n","                            IGNORE_LABEL)\n","\n","validate(segmenter, val_loader, 0, num_classes=NUM_CLASSES[task_idx])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:__main__: Created train set = 699 examples, val set = 100 examples\n","INFO:__main__: IoUs: [0.91260202 0.69896981]\n","INFO:__main__: Mean time: 0.038830628395080564\n","INFO:__main__: Val epoch: 0\tMean IoU: 0.806\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0.8057859133688148"]},"metadata":{"tags":[]},"execution_count":113}]}]}